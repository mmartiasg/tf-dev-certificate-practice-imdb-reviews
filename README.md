Implementation will be carried on 
 - Keras
 - Tensorflow 2
 - JAX
 - Pythorch
 
Further experiments:
  - How does the number of words affect the algorithm capacity to learn
  - Or maybe increase or decrease the max sequence has a negative impact at some point
  - Network size is better to have a bigger one? How will that overfit the dataset?
  - So far I'm using a onw hot encoding I wonder if using a TF-idf will help because I only have if that token appers or not but all of them weight the same!
  - Maybe embeddings will make an improvement?
  - Another model like LSTM, GRU or Transformers could be better for this task and if that is how much better?
